{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import re\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\\\machine_learning_data\\\\train.csv\"\n",
    "raw_data_train = pd.read_csv(file_path)\n",
    "file_path = \"D:\\\\machine_learning_data\\\\test.csv\"\n",
    "raw_data_test = pd.read_csv(file_path)\n",
    "real_test_data = pd.read_csv(\"D:\\\\machine_learning_data\\\\data_real.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "#real_test_data = real_test_data.drop(real_test_data.index[0])\n",
    "real_test_data = real_test_data.drop(real_test_data.index[288])\n",
    "real_test_data = real_test_data.drop(real_test_data.index[287])\n",
    "real_test_data = real_test_data.drop(real_test_data.index[270])\n",
    "translator = Translator()\n",
    "\n",
    "count = 0\n",
    "for element in real_test_data['post_description']:\n",
    "    print(count)\n",
    "    real_test_data.iloc[count, 0] = translator.translate(deEmojify(element)).text\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = raw_data_train\n",
    "data_test = raw_data_test\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "def split(string, start, end):\n",
    "    return ' '.join(string.split()[start:end])\n",
    "\n",
    "# cleaning training dataset\n",
    "\n",
    "data_train['post'] = data_train['post'].str.lower()\n",
    "#data_train['post'] = data_train['post'].apply(cleanHtml)\n",
    "data_train['post'] = data_train['post'].apply(cleanPunc)\n",
    "data_train['post'] = data_train['post'].apply(keepAlpha)\n",
    "\n",
    "count = 0\n",
    "for str in data_train['post']:\n",
    "    data_train.iloc[count, 0] = split(str, 0, 50)\n",
    "    count = count + 1\n",
    "    \n",
    "# cleaning test dataset\n",
    "\n",
    "data_test['post'] = data_test['post'].str.lower()\n",
    "#data_test['post'] = data_test['post'].apply(cleanHtml)\n",
    "data_test['post'] = data_test['post'].apply(cleanPunc)\n",
    "data_test['post'] = data_test['post'].apply(keepAlpha)\n",
    "\n",
    "count = 0\n",
    "for str in data_test['post']:\n",
    "    data_test.iloc[count, 0] = split(str, 0, 50)\n",
    "    count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten',\n",
    "#                   'may','also','across','among','beside','however','yet','within'])\n",
    "#re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "new_stop_words = ['zero','one','two','three','four','five','six','seven','eight','nine','ten',\n",
    "                  'may','also','across','among','beside','however','yet','within', 'r', 'function', 'code', \n",
    "                  'library', 'libraries']\n",
    "stop_words = stop_words.union(new_stop_words)\n",
    "\n",
    "not_stopwords = {'which', 'when', 'how', 'but', 'should', 'where', 'who', 'whom', 'what'}\n",
    "stop_words = set([word for word in stop_words if word not in not_stopwords])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "data_train[\"post\"] = data_train[\"post\"].apply(removeStopWords)\n",
    "data_test[\"post\"] = data_test[\"post\"].apply(removeStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "data_train['post'] = data_train['post'].apply(stemming)\n",
    "data_test['post'] = data_test['post'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#train, test = train_test_split(data, random_state=42, test_size=0.30, shuffle=True)\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "\n",
    "train_text = data_train['post']\n",
    "test_text = data_test['post']\n",
    "\n",
    "vectorizer.fit_transform(train_text)\n",
    "vectorizer.fit_transform(test_text)\n",
    "\n",
    "x_train = vectorizer.transform(train_text)\n",
    "y_train = data_train.drop(labels = ['post'], axis=1)\n",
    "\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = data_test.drop(labels = ['post'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiClass Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "categories = list(raw_data_train.columns.values)\n",
    "categories = categories[1:]\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "for category in categories:\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, data_train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction_test = LogReg_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(data_test[category], prediction_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# initialize classifier chains multi-label classifier\n",
    "classifier = ClassifierChain(LogisticRegression())\n",
    "# Training logistic regression model on train data\n",
    "classifier.fit(x_train, y_train)\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using MLkNN\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "classifier_new = MLkNN(k=10)\n",
    "x_train = lil_matrix(x_train).toarray()\n",
    "y_train = lil_matrix(y_train).toarray()\n",
    "x_test = lil_matrix(x_test).toarray()\n",
    "# train\n",
    "classifier_new.fit(x_train, y_train)\n",
    "# predict\n",
    "predictions_new = classifier_new.predict(x_test)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the predictions for the real dataset\n",
    "data = real_test_data\n",
    "\n",
    "data['post_description'] = data['post_description'].str.lower()\n",
    "# data['post_description'] = data['post_description'].apply(cleanHtml)\n",
    "data['post_description'] = data['post_description'].apply(cleanPunc)\n",
    "data['post_description'] = data['post_description'].apply(keepAlpha)\n",
    "data['post_description'] = data['post_description'].apply(removeStopWords)\n",
    "data['post_description'] = data['post_description'].apply(stemming)\n",
    "# print(data)\n",
    "count = 0\n",
    "\n",
    "for str in data['post_description']:\n",
    "    data.iloc[count, 0] = split(str, 0, 50)\n",
    "    count = count + 1\n",
    "\n",
    "real_text = data['post_description']\n",
    "print(real_text)\n",
    "vectorizer.fit_transform(real_text)\n",
    "print(real_text)\n",
    "test_data = vectorizer.transform(real_text)\n",
    "prediction_real_data = LogReg_pipeline.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data)\n",
    "print(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific",
   "language": "python",
   "name": "scientific"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
